{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries and Load Data**\n",
        "\n",
        "To start, I imported essential libraries for data handling, visualization, preprocessing, modeling, and evaluation. Each library plays a specific role in managing different parts of the project. For example:\n",
        "\n",
        "Pandas and NumPy are used for data manipulation and basic operations.\n",
        "Seaborn and Matplotlib enable data visualization, helping me understand distributions, correlations, and patterns.\n",
        "Scikit-learn provides tools for preprocessing, feature selection, model building, and evaluation.\n",
        "After importing the libraries, I loaded the dataset and checked for missing values and data types in each column. This step allowed me to gain a preliminary understanding of the data structure and to identify any immediate issues that might need addressing, such as missing values or categorical features.\n",
        "\n"
      ],
      "metadata": {
        "id": "HHGbCm5rHvf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.experimental import enable_iterative_imputer  # Enable the experimental feature\n",
        "from sklearn.impute import IterativeImputer  # Import IterativeImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Breast_Cancer_dataset.csv')\n",
        "\n",
        "# Display initial data information\n",
        "print(\"Dataset Info:\")\n",
        "print(data.info())\n",
        "print(\"Missing values per column:\\n\", data.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl_84JuTXIur",
        "outputId": "8ca6ba37-78b8-40ed-ac48-a42a89a6ad1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4024 entries, 0 to 4023\n",
            "Data columns (total 16 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   Age                     3823 non-null   float64\n",
            " 1   Race                    3622 non-null   object \n",
            " 2   Marital Status          3703 non-null   object \n",
            " 3   T Stage                 4024 non-null   object \n",
            " 4   N Stage                 4024 non-null   object \n",
            " 5   6th Stage               4024 non-null   object \n",
            " 6   differentiate           4024 non-null   object \n",
            " 7   Grade                   4024 non-null   object \n",
            " 8   A Stage                 4024 non-null   object \n",
            " 9   Tumor Size              3622 non-null   float64\n",
            " 10  Estrogen Status         3823 non-null   object \n",
            " 11  Progesterone Status     4024 non-null   object \n",
            " 12  Regional Node Examined  3421 non-null   float64\n",
            " 13  Reginol Node Positive   4024 non-null   int64  \n",
            " 14  Survival Months         4024 non-null   int64  \n",
            " 15  Status                  4024 non-null   object \n",
            "dtypes: float64(3), int64(2), object(11)\n",
            "memory usage: 503.1+ KB\n",
            "None\n",
            "Missing values per column:\n",
            " Age                       201\n",
            "Race                      402\n",
            "Marital Status            321\n",
            "T Stage                     0\n",
            "N Stage                     0\n",
            "6th Stage                   0\n",
            "differentiate               0\n",
            "Grade                       0\n",
            "A Stage                     0\n",
            "Tumor Size                402\n",
            "Estrogen Status           201\n",
            "Progesterone Status         0\n",
            "Regional Node Examined    603\n",
            "Reginol Node Positive       0\n",
            "Survival Months             0\n",
            "Status                      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing (Handle Missing Values and Encode Categorical Variables)**\n",
        "\n",
        "\n",
        "In this chunk, I focused on preparing the dataset for model training by addressing two primary issues: categorical variables and missing values.\n",
        "\n",
        "Encoding Categorical Variables: Since many machine learning algorithms require numerical input, I converted categorical columns to numeric codes using Label Encoding. This process maps each unique category to an integer, allowing models to interpret these categories without adding undue complexity.\n",
        "\n",
        "Handling Missing Values with MICE: To handle missing values effectively, I used Multiple Imputation by Chained Equations (MICE). MICE iteratively fills in missing values by modeling each feature with missing values as a function of other features. This advanced imputation approach preserves the data’s integrity and avoids the bias that might result from simpler methods.\n",
        "\n",
        "After these steps, I confirmed that the dataset had no missing values left. This preprocessing step is crucial because models often perform poorly on incomplete data."
      ],
      "metadata": {
        "id": "LLBbsMWPH3F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "for col in data.select_dtypes(include='object').columns:\n",
        "    data[col] = LabelEncoder().fit_transform(data[col].astype(str))\n",
        "\n",
        "# Handle missing values using MICE (Iterative Imputer)\n",
        "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "data_imputed = imputer.fit_transform(data)\n",
        "data = pd.DataFrame(data_imputed, columns=data.columns)\n",
        "\n",
        "# Check if missing values are handled\n",
        "print(\"Missing values after imputation:\\n\", data.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHCq78Zea5k1",
        "outputId": "19cadb78-95ba-4d83-80e7-dfb2d6fabe52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            " Age                       0\n",
            "Race                      0\n",
            "Marital Status            0\n",
            "T Stage                   0\n",
            "N Stage                   0\n",
            "6th Stage                 0\n",
            "differentiate             0\n",
            "Grade                     0\n",
            "A Stage                   0\n",
            "Tumor Size                0\n",
            "Estrogen Status           0\n",
            "Progesterone Status       0\n",
            "Regional Node Examined    0\n",
            "Reginol Node Positive     0\n",
            "Survival Months           0\n",
            "Status                    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier Detection and Removal**\n",
        "\n",
        "\n",
        "Next, I addressed the issue of outliers, which can distort model training and lead to inaccurate predictions. I used Z-score analysis to detect extreme values. A Z-score measures how far each value is from the mean, in terms of standard deviations. I considered values with a Z-score greater than 3 to be outliers, indicating that they are quite far from typical values in the dataset.\n",
        "\n",
        "After identifying outliers, I removed them to ensure the dataset was representative of general cases, improving model robustness and reducing the risk of overfitting to noise.\n",
        "\n"
      ],
      "metadata": {
        "id": "MfTfp-1eIHGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect and remove outliers using Z-score\n",
        "z_scores = np.abs((data - data.mean()) / data.std())\n",
        "data = data[(z_scores < 3).all(axis=1)]  # Removing outliers beyond 3 standard deviations\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['Status'])  # Adjust 'Status' if your target column has a different name\n",
        "y = data['Status']\n"
      ],
      "metadata": {
        "id": "edbIRTLMbAts"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardization and Dimensionality Reduction (PCA)**\n",
        "\n",
        "\n",
        "Standardization: Before applying machine learning models, I standardized the dataset so that each feature had a mean of 0 and a standard deviation of 1. Standardization is essential for algorithms that are sensitive to feature scales, such as KNN and Neural Networks. It ensures that all features contribute equally, avoiding bias toward features with larger scales.\n",
        "\n",
        "Principal Component Analysis (PCA): Since high-dimensional data can lead to redundancy and slow computation, I applied PCA to reduce the dataset’s dimensionality while preserving 95% of the variance. PCA transforms the data into new features (principal components) that capture the most important patterns. This step streamlines the dataset, minimizes noise, and can improve model performance by focusing only on the most informative aspects of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dwiq6b6EIQGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensionality while retaining 95% variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(f\"Number of features after PCA: {X_pca.shape[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_F4mUpqbEsq",
        "outputId": "0e545dbd-9d5b-4b54-8892-5bd425d6deb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features after PCA: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recursive Feature Elimination (RFE)**\n",
        "\n",
        "\n",
        "In this chunk, I used Recursive Feature Elimination (RFE) to select the most important features. RFE is a powerful feature selection technique that iteratively removes the least significant features and retrains the model with the remaining ones. For this project, I used a Random Forest model as the estimator within RFE.\n",
        "\n",
        "The Random Forest model helped assess feature importance during each iteration. By specifying that I wanted to select the top 5 features, RFE ranked the features and retained only those that contributed most to model performance. This method is beneficial for reducing dimensionality in a targeted way, ensuring the model focuses on the most predictive features.\n",
        "\n"
      ],
      "metadata": {
        "id": "jISgxHHFIZXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest model for RFE\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Apply Recursive Feature Elimination to select top features\n",
        "rfe_selector = RFE(estimator=model, n_features_to_select=5, step=1)\n",
        "rfe_selector.fit(X_scaled, y)\n",
        "\n",
        "# Get selected features based on RFE\n",
        "rfe_features = X.columns[rfe_selector.get_support()]\n",
        "X_selected = X[rfe_features]\n",
        "print(\"Selected features with RFE:\", rfe_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCBQxFhjbKxt",
        "outputId": "30825c89-5df2-4580-aae6-183b17044ce6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features with RFE: Index(['Age', 'Tumor Size', 'Regional Node Examined', 'Reginol Node Positive',\n",
            "       'Survival Months'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Data into Training and Testing Sets**\n",
        "\n",
        "With the most important features selected, I split the data into training and testing sets. This separation is essential for assessing model performance objectively. The training set (80% of the data) is used to fit the model, while the testing set (20%) is kept separate to evaluate how well the model generalizes to unseen data.\n",
        "\n",
        "By using this approach, I could assess how the model might perform on new cases, ensuring that my results reflect generalizability rather than overfitting to the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "W3WYU1h2Iq_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets using selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reset indices to ensure alignment\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "2I3EUTjKbQ9q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**\n",
        "\n",
        "\n",
        "Here, I implemented and evaluated six different machine learning models, each with unique strengths and limitations:\n",
        "\n",
        "*K-Nearest Neighbors (KNN):* I implemented KNN from scratch to gain a deeper understanding of the algorithm. KNN works by finding the nearest k neighbors of a data point and predicting the majority class. It is simple, interpretable, and effective for low-dimensional data. However, it’s sensitive to feature scaling and can be slow with large datasets.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JrTUr9tsIu6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define Euclidean distance function\n",
        "def euclidean_distance(a, b):\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "# Define custom KNN class\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Ensure that the data is in Numpy array format\n",
        "        self.X_train = np.array(X)\n",
        "        self.y_train = np.array(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Convert the input data to a Numpy array\n",
        "        X = np.array(X)\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Compute distances from x to all training samples\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        # Get indices of the k nearest neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        # Extract the labels of the k nearest neighbors\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Return the most common label among the neighbors\n",
        "        return np.argmax(np.bincount(k_nearest_labels))\n",
        "\n",
        "# Reset index after preprocessing steps (if applicable)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Redefine X and y after any preprocessing that may alter the indices\n",
        "X = data.drop(columns=['Status'])  # Replace 'Status' with actual target column name if different\n",
        "y = data['Status']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert training and test sets to numpy arrays\n",
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values\n",
        "X_test_np = X_test.values\n",
        "\n",
        "# Instantiate, fit, and predict with the custom KNN model\n",
        "knn_model = KNN(k=5)\n",
        "knn_model.fit(X_train_np, y_train_np)\n",
        "knn_predictions = knn_model.predict(X_test_np)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"KNN Accuracy:\", accuracy_score(y_test, knn_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b02HyAr9c97g",
        "outputId": "aa499f85-d475-47f8-8bd3-c2660d8a38ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.8887323943661972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Naïve Bayes:* This probabilistic model is based on Bayes’ theorem and assumes feature independence. Naïve Bayes is fast and handles small datasets well, but it may underperform if features are highly correlated.\n",
        "\n",
        "*Decision Tree:* The Decision Tree model splits data based on feature conditions, forming branches that lead to class predictions. It’s highly interpretable and can handle non-linear relationships, though it’s prone to overfitting without pruning.\n",
        "\n",
        "*Random Forest:* Random Forest is an ensemble method that builds multiple decision trees and combines their predictions. It reduces overfitting and improves accuracy, particularly for large datasets. However, it’s less interpretable compared to a single decision tree.\n",
        "\n",
        "*Gradient Boosting:* Gradient Boosting iteratively improves predictions by learning from errors in previous models. This makes it highly accurate and suitable for complex patterns, although it requires careful tuning to avoid overfitting.\n",
        "\n",
        "*Neural Network:* Lastly, I implemented a Neural Network with two hidden layers. Neural networks excel at capturing complex patterns in large datasets. However, they’re computationally intensive and require substantial tuning to perform well.\n",
        "\n",
        "Each model was evaluated on the testing set, allowing me to compare accuracy across different approaches."
      ],
      "metadata": {
        "id": "wdDRjPkNJEz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2. Naïve Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "print(\"Naïve Bayes Accuracy:\", accuracy_score(y_test, nb_predictions))\n",
        "\n",
        "# 3. Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_predictions))\n",
        "\n",
        "# 4. Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
        "\n",
        "# 5. Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_predictions = gb_model.predict(X_test)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb_predictions))\n",
        "\n",
        "# 6. Neural Network\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "nn_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test)\n",
        "print(\"Neural Network Accuracy:\", nn_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A802nm28bWa7",
        "outputId": "99d83c21-4d34-4c25-d7a2-31aefbcb4904"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naïve Bayes Accuracy: 0.8098591549295775\n",
            "Decision Tree Accuracy: 0.8380281690140845\n",
            "Random Forest Accuracy: 0.9014084507042254\n",
            "Gradient Boosting Accuracy: 0.9028169014084507\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8509 - loss: 0.4827\n",
            "Epoch 2/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8893 - loss: 0.3100\n",
            "Epoch 3/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9071 - loss: 0.2778\n",
            "Epoch 4/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8972 - loss: 0.2888\n",
            "Epoch 5/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8950 - loss: 0.3089\n",
            "Epoch 6/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.2760\n",
            "Epoch 7/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9005 - loss: 0.2900\n",
            "Epoch 8/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9061 - loss: 0.2719\n",
            "Epoch 9/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9061 - loss: 0.2680\n",
            "Epoch 10/10\n",
            "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9034 - loss: 0.2713\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8787 - loss: 0.3111\n",
            "Neural Network Accuracy: 0.8845070600509644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further optimize model performance, I conducted **hyperparameter tuning** on two of the models: Random Forest and Gradient Boosting.\n",
        "\n",
        "\n",
        "\n",
        "Grid Search for Random Forest: I adjusted parameters like n_estimators (number of trees) and max_depth (depth of each tree) using a grid search. Grid search systematically evaluates each combination to identify the configuration that maximizes cross-validation accuracy.\n",
        "\n",
        "Grid Search for Gradient Boosting: For Gradient Boosting, I tuned n_estimators and learning_rate. The learning rate controls how much each tree contributes to the final model, while n_estimators determines the number of trees. By fine-tuning these parameters, I was able to improve model accuracy and generalization.\n",
        "\n",
        "These tuning efforts allowed me to identify the best-performing configurations for each model, enhancing their predictive power.\n",
        "\n"
      ],
      "metadata": {
        "id": "H-DucZgAJu0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for Random Forest and Gradient Boosting\n",
        "param_grid_rf = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\n",
        "grid_rf.fit(X_train, y_train)\n",
        "print(\"Best Random Forest Params:\", grid_rf.best_params_)\n",
        "print(\"Random Forest best accuracy:\", grid_rf.best_score_)\n",
        "\n",
        "param_grid_gb = {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 0.5]}\n",
        "grid_gb = GridSearchCV(GradientBoostingClassifier(), param_grid_gb, cv=5)\n",
        "grid_gb.fit(X_train, y_train)\n",
        "print(\"Best Gradient Boosting Params:\", grid_gb.best_params_)\n",
        "print(\"Gradient Boosting best accuracy:\", grid_gb.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF9SLFyEdKAu",
        "outputId": "8e09c8ed-c41a-4916-abbf-936fd231e9c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest Params: {'max_depth': None, 'n_estimators': 50}\n",
            "Random Forest best accuracy: 0.9111371935315598\n",
            "Best Gradient Boosting Params: {'learning_rate': 0.1, 'n_estimators': 50}\n",
            "Gradient Boosting best accuracy: 0.9129002409518842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Results**\n",
        "\n",
        "\n",
        "Finally, I summarized the accuracy of each model in a results table, providing a straightforward comparison of performance across models. This table highlights which models performed best on the test set, making it easy to identify the most reliable model for predicting breast cancer survivability.\n",
        "\n",
        "Additionally, for models like Random Forest that provide feature importance scores, I highlighted the most influential features. This information is valuable for understanding which patient characteristics are most predictive of survivability, providing insights that could be valuable in a clinical setting.\n",
        "\n",
        "In conclusion, this project demonstrates a systematic approach to predictive modeling, covering data preprocessing, feature selection, model implementation, and tuning. Each step was carefully designed to improve model performance and interpretability, resulting in a reliable and accurate tool for predicting breast cancer survivability."
      ],
      "metadata": {
        "id": "YUeFvv6CJ6ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best models from GridSearchCV on the test set\n",
        "best_rf_model = grid_rf.best_estimator_\n",
        "best_rf_predictions = best_rf_model.predict(X_test)\n",
        "best_rf_accuracy = accuracy_score(y_test, best_rf_predictions)\n",
        "\n",
        "best_gb_model = grid_gb.best_estimator_\n",
        "best_gb_predictions = best_gb_model.predict(X_test)\n",
        "best_gb_accuracy = accuracy_score(y_test, best_gb_predictions)\n",
        "\n",
        "# Display results for all models, including the best-tuned RF and GB models\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\n",
        "        \"KNN\",\n",
        "        \"Naïve Bayes\",\n",
        "        \"Decision Tree\",\n",
        "        \"Random Forest (Default)\",\n",
        "        \"Gradient Boosting (Default)\",\n",
        "        \"Neural Network\",\n",
        "        \"Random Forest (Tuned)\",\n",
        "        \"Gradient Boosting (Tuned)\"\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_test, knn_predictions),\n",
        "        accuracy_score(y_test, nb_predictions),\n",
        "        accuracy_score(y_test, dt_predictions),\n",
        "        accuracy_score(y_test, rf_predictions),          # Default RF accuracy\n",
        "        accuracy_score(y_test, gb_predictions),          # Default GB accuracy\n",
        "        nn_accuracy,\n",
        "        best_rf_accuracy,                                # Tuned RF accuracy\n",
        "        best_gb_accuracy                                 # Tuned GB accuracy\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nModel Performance Summary:\\n\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2erlcUFedWj0",
        "outputId": "42783d0e-3940-4c80-bbbd-9d0fe83d6f18"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance Summary:\n",
            "                          Model  Accuracy\n",
            "0                          KNN  0.888732\n",
            "1                  Naïve Bayes  0.809859\n",
            "2                Decision Tree  0.838028\n",
            "3      Random Forest (Default)  0.901408\n",
            "4  Gradient Boosting (Default)  0.902817\n",
            "5               Neural Network  0.884507\n",
            "6        Random Forest (Tuned)  0.907042\n",
            "7    Gradient Boosting (Tuned)  0.898592\n"
          ]
        }
      ]
    }
  ]
}